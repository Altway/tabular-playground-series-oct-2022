{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# TODO \n",
    "# - TF functionnal API\n",
    "# - Explorer tes donnees pour sortir des tendances\n",
    "# - Dans un softmax = pas de NaN == donnee manquante dans les data de test\n",
    "# - Data symetrique pour les situation de but donc run le model sur A seul = proba de A, run le model sur B = proba de B \n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### META\n",
    "# PATH\n",
    "DATA_FOLDER = \"./data\"\n",
    "CHECKPOINTS_FOLDER = \"./checkpoints\"\n",
    "CHECKPOINTS_PATH = CHECKPOINTS_FOLDER+\"/cp-{epoch:04d}.ckpt\"\n",
    "OUT_FOLDER = \"./out\"\n",
    "\n",
    "# MODEL\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 0 ns, total: 12 µs\n",
      "Wall time: 15.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "        iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 74.89 Mb (44.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Load dtype model to reduce size of df\n",
    "dtypes_df = pd.read_csv(DATA_FOLDER+\"/train_dtypes.csv\")\n",
    "dtypes = {k: v for (k, v) in zip(dtypes_df.column, dtypes_df.dtype)}\n",
    "\n",
    "# Load TEST DF\n",
    "test_df = pd.read_csv(f\"{DATA_FOLDER}/test.csv\", dtype=dtypes)\n",
    "test_df.fillna(-1000.0, inplace=True)\n",
    "test_df = reduce_memory_usage(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing 0 df\n",
      "Mem. usage decreased to 260.33 Mb (40.9% reduction)\n",
      "Optimizing 1 df\n",
      "Mem. usage decreased to 262.67 Mb (40.9% reduction)\n",
      "Optimizing 2 df\n",
      "Mem. usage decreased to 257.43 Mb (40.9% reduction)\n",
      "Optimizing 3 df\n",
      "Mem. usage decreased to 256.11 Mb (40.9% reduction)\n",
      "Optimizing 4 df\n",
      "Mem. usage decreased to 256.51 Mb (40.9% reduction)\n",
      "Optimizing 5 df\n",
      "Mem. usage decreased to 253.59 Mb (40.9% reduction)\n",
      "Optimizing 6 df\n",
      "Mem. usage decreased to 249.98 Mb (40.9% reduction)\n",
      "Optimizing 7 df\n",
      "Mem. usage decreased to 255.51 Mb (40.9% reduction)\n",
      "Optimizing 8 df\n",
      "Mem. usage decreased to 260.62 Mb (40.9% reduction)\n",
      "Optimizing 9 df\n",
      "Mem. usage decreased to 254.69 Mb (40.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Load TRAIN DF\n",
    "train_df_list = []\n",
    "for i in range(10):\n",
    "    print(f\"Optimizing {i} df\")\n",
    "    train_df_list.append(reduce_memory_usage(pd.read_csv(f\"{DATA_FOLDER}/train_{i}.csv\", dtype=dtypes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 512)               28160     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,226\n",
      "Trainable params: 194,434\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "\n",
    "    model_v1 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=[54]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model_v2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=[54]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model_v3 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=[54]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model_v1.compile(\n",
    "        optimizer='adam',\n",
    "        #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['AUC', 'accuracy']\n",
    "    )\n",
    "    # Accuracy pas pertinente: ici le \"recall\" = taux de hit des but est plus pertinent\n",
    "    # AUC/Courbe ROC plus pertinent pour la detection des faux/vrai positifs\n",
    "    model_v2.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['AUC', tf.keras.metrics.Recall()]\n",
    "    )   \n",
    "\n",
    "    model_v3.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['AUC', tf.keras.metrics.Recall()]\n",
    "    )   \n",
    "        \n",
    "    return model_v3\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 512)               28160     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,226\n",
      "Trainable params: 194,434\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS CONFIGURATION\n",
    "\n",
    "class AccuracyCallback(tf.keras.callbacks.Callback):\n",
    "        # Define the correct function signature for on_epoch_end\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            accuracy = 0.93\n",
    "            if logs.get('accuracy') is not None and logs.get('accuracy') > accuracy:\n",
    "                print(f\"\\nReached {accuracy*100}% accuracy so cancelling training!\") \n",
    "                \n",
    "                # Stop training once the above condition is met\n",
    "                self.model.stop_training = True\n",
    "\n",
    "class RecallCallback(tf.keras.callbacks.Callback):\n",
    "        # Define the correct function signature for on_epoch_end\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            recall = 0.93\n",
    "            if logs.get('recall') is not None and logs.get('recall') > recall:\n",
    "                print(f\"\\nReached {recall*100}%  so cancelling training!\") \n",
    "                \n",
    "                # Stop training once the above condition is met\n",
    "acc_callback = AccuracyCallback()\n",
    "rec_callback = RecallCallback()\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CHECKPOINTS_PATH,\n",
    "    save_weights_only=True,\n",
    "    save_freq=150*BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "# Scheduler plateau pour le learning rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20)\n",
    ")\n",
    "\n",
    "callbacks_list = [rec_callback]#, lr_schedule, cp_callback, acc_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset number 0\n",
      "Epoch 1/3\n",
      "15113/15113 [==============================] - 173s 11ms/step - loss: 0.2018 - auc: 0.7155 - recall_6: 0.0313 - val_loss: 0.1901 - val_auc: 0.7600 - val_recall_6: 0.0324\n",
      "Epoch 2/3\n",
      "15113/15113 [==============================] - 155s 10ms/step - loss: 0.1855 - auc: 0.7783 - recall_6: 0.0684 - val_loss: 0.1733 - val_auc: 0.8245 - val_recall_6: 0.0766\n",
      "Epoch 3/3\n",
      "15113/15113 [==============================] - 148s 10ms/step - loss: 0.1703 - auc: 0.8316 - recall_6: 0.1188 - val_loss: 0.1520 - val_auc: 0.8779 - val_recall_6: 0.1599\n",
      "\n",
      "Training dataset number 1\n",
      "Epoch 1/3\n",
      "15250/15250 [==============================] - 133s 9ms/step - loss: 0.1812 - auc: 0.7703 - recall_6: 0.0518 - val_loss: 0.1626 - val_auc: 0.8403 - val_recall_6: 0.0718\n",
      "Epoch 2/3\n",
      "15250/15250 [==============================] - 122s 8ms/step - loss: 0.1606 - auc: 0.8439 - recall_6: 0.1254 - val_loss: 0.1393 - val_auc: 0.8982 - val_recall_6: 0.1679\n",
      "Epoch 3/3\n",
      "15250/15250 [==============================] - 110s 7ms/step - loss: 0.1461 - auc: 0.8804 - recall_6: 0.1999 - val_loss: 0.1237 - val_auc: 0.9272 - val_recall_6: 0.2235\n",
      "\n",
      "Training dataset number 2\n",
      "Epoch 1/3\n",
      "14945/14945 [==============================] - 110s 7ms/step - loss: 0.1821 - auc: 0.7737 - recall_6: 0.0612 - val_loss: 0.1598 - val_auc: 0.8515 - val_recall_6: 0.1038\n",
      "Epoch 2/3\n",
      "14945/14945 [==============================] - 89s 6ms/step - loss: 0.1596 - auc: 0.8504 - recall_6: 0.1421 - val_loss: 0.1360 - val_auc: 0.9089 - val_recall_6: 0.1556\n",
      "Epoch 3/3\n",
      "14945/14945 [==============================] - 104s 7ms/step - loss: 0.1448 - auc: 0.8853 - recall_6: 0.2233 - val_loss: 0.1199 - val_auc: 0.9330 - val_recall_6: 0.2569\n",
      "\n",
      "Training dataset number 3\n",
      "Epoch 1/3\n",
      "14869/14869 [==============================] - 90s 6ms/step - loss: 0.1883 - auc: 0.7660 - recall_6: 0.0602 - val_loss: 0.1679 - val_auc: 0.8460 - val_recall_6: 0.0985\n",
      "Epoch 2/3\n",
      "14869/14869 [==============================] - 88s 6ms/step - loss: 0.1657 - auc: 0.8449 - recall_6: 0.1382 - val_loss: 0.1434 - val_auc: 0.9023 - val_recall_6: 0.1731\n",
      "Epoch 3/3\n",
      "14869/14869 [==============================] - 94s 6ms/step - loss: 0.1508 - auc: 0.8804 - recall_6: 0.2142 - val_loss: 0.1269 - val_auc: 0.9306 - val_recall_6: 0.2478\n",
      "\n",
      "Training dataset number 4\n",
      "Epoch 1/3\n",
      "14892/14892 [==============================] - 88s 6ms/step - loss: 0.1832 - auc: 0.7690 - recall_6: 0.0572 - val_loss: 0.1619 - val_auc: 0.8486 - val_recall_6: 0.0931\n",
      "Epoch 2/3\n",
      "14892/14892 [==============================] - 88s 6ms/step - loss: 0.1608 - auc: 0.8477 - recall_6: 0.1362 - val_loss: 0.1375 - val_auc: 0.9053 - val_recall_6: 0.1725\n",
      "Epoch 3/3\n",
      "14892/14892 [==============================] - 140s 9ms/step - loss: 0.1461 - auc: 0.8828 - recall_6: 0.2147 - val_loss: 0.1188 - val_auc: 0.9342 - val_recall_6: 0.2930\n",
      "\n",
      "Training dataset number 5\n",
      "Epoch 1/3\n",
      "14722/14722 [==============================] - 140s 9ms/step - loss: 0.1879 - auc: 0.7631 - recall_6: 0.0558 - val_loss: 0.1670 - val_auc: 0.8422 - val_recall_6: 0.0845\n",
      "Epoch 2/3\n",
      "14722/14722 [==============================] - 143s 10ms/step - loss: 0.1657 - auc: 0.8417 - recall_6: 0.1304 - val_loss: 0.1418 - val_auc: 0.9028 - val_recall_6: 0.1822\n",
      "Epoch 3/3\n",
      "14722/14722 [==============================] - 131s 9ms/step - loss: 0.1507 - auc: 0.8784 - recall_6: 0.2068 - val_loss: 0.1248 - val_auc: 0.9306 - val_recall_6: 0.2528\n",
      "\n",
      "Training dataset number 6\n",
      "Epoch 1/3\n",
      "14513/14513 [==============================] - 112s 8ms/step - loss: 0.1924 - auc: 0.7616 - recall_6: 0.0614 - val_loss: 0.1694 - val_auc: 0.8404 - val_recall_6: 0.1033\n",
      "Epoch 2/3\n",
      "14513/14513 [==============================] - 111s 8ms/step - loss: 0.1704 - auc: 0.8393 - recall_6: 0.1331 - val_loss: 0.1446 - val_auc: 0.8996 - val_recall_6: 0.1826\n",
      "Epoch 3/3\n",
      "14513/14513 [==============================] - 109s 8ms/step - loss: 0.1559 - auc: 0.8748 - recall_6: 0.2047 - val_loss: 0.1266 - val_auc: 0.9297 - val_recall_6: 0.2688\n",
      "\n",
      "Training dataset number 7\n",
      "Epoch 1/3\n",
      "14834/14834 [==============================] - 114s 8ms/step - loss: 0.1884 - auc: 0.7598 - recall_6: 0.0540 - val_loss: 0.1680 - val_auc: 0.8375 - val_recall_6: 0.0783\n",
      "Epoch 2/3\n",
      "14834/14834 [==============================] - 86s 6ms/step - loss: 0.1677 - auc: 0.8363 - recall_6: 0.1206 - val_loss: 0.1442 - val_auc: 0.8976 - val_recall_6: 0.1456\n",
      "Epoch 3/3\n",
      "14834/14834 [==============================] - 110s 7ms/step - loss: 0.1531 - auc: 0.8735 - recall_6: 0.1913 - val_loss: 0.1270 - val_auc: 0.9262 - val_recall_6: 0.2356\n",
      "\n",
      "Training dataset number 8\n",
      "Epoch 1/3\n",
      "15131/15131 [==============================] - 164s 11ms/step - loss: 0.1859 - auc: 0.7631 - recall_6: 0.0567 - val_loss: 0.1668 - val_auc: 0.8367 - val_recall_6: 0.0856\n",
      "Epoch 2/3\n",
      "15131/15131 [==============================] - 163s 11ms/step - loss: 0.1657 - auc: 0.8376 - recall_6: 0.1199 - val_loss: 0.1432 - val_auc: 0.8964 - val_recall_6: 0.1606\n",
      "Epoch 3/3\n",
      "15131/15131 [==============================] - 172s 11ms/step - loss: 0.1515 - auc: 0.8740 - recall_6: 0.1885 - val_loss: 0.1264 - val_auc: 0.9261 - val_recall_6: 0.2553\n",
      "\n",
      "Training dataset number 9\n",
      "Epoch 1/3\n",
      "14786/14786 [==============================] - 166s 11ms/step - loss: 0.1893 - auc: 0.7554 - recall_6: 0.0540 - val_loss: 0.1695 - val_auc: 0.8306 - val_recall_6: 0.0780\n",
      "Epoch 2/3\n",
      "14786/14786 [==============================] - 151s 10ms/step - loss: 0.1692 - auc: 0.8316 - recall_6: 0.1141 - val_loss: 0.1458 - val_auc: 0.8922 - val_recall_6: 0.1511\n",
      "Epoch 3/3\n",
      "14786/14786 [==============================] - 148s 10ms/step - loss: 0.1550 - auc: 0.8686 - recall_6: 0.1826 - val_loss: 0.1289 - val_auc: 0.9224 - val_recall_6: 0.2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_remove = [\n",
    "    'game_num', \n",
    "    'event_id', \n",
    "    'event_time', \n",
    "    'player_scoring_next', \n",
    "    'team_scoring_next',\n",
    "]\n",
    "\n",
    "history_list = []\n",
    "\n",
    "model.save_weights(CHECKPOINTS_PATH.format(epoch=0))\n",
    "\n",
    "for i, df in enumerate(train_df_list):\n",
    "    #df['no_team_scored'] = np.logical_xor(df['team_A_scoring_within_10sec'],df['team_B_scoring_within_10sec'])\n",
    "    #df['no_team_scored'] = (~df['no_team_scored']).astype(int)\n",
    "        \n",
    "    df_drop = df.drop(to_remove, axis=1)\n",
    "    # improvment: fillna pour prendre en compte les destructions et eviter les NaN dans les predictions\n",
    "    #df_drop = df.dropna(inplace=False, axis=0)\n",
    "\n",
    "    df_drop = df_drop.fillna(-1000.0)\n",
    "\n",
    "        \n",
    "    features = df_drop.loc[:,'ball_pos_x':'boost5_timer']\n",
    "    #target = df_drop.loc[:,'team_A_scoring_within_10sec':'no_team_scored']\n",
    "    target = df_drop.loc[:,'team_A_scoring_within_10sec':'team_B_scoring_within_10sec']\n",
    "\n",
    "    # test_size en regle general = 10/15 % # previous: 0.05\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(features, target, test_size = 0.10, random_state=41)\n",
    "        \n",
    "    print(f'Training dataset number {i}')\n",
    "        \n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        Y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        callbacks=callbacks_list,\n",
    "    )\n",
    "\n",
    "    # can we plot learning history on multiple training ?\n",
    "    history_list.append(history)\n",
    "    \n",
    "    # memory management to lower chance of kernel crashing\n",
    "    del df, X_train, X_val, Y_train, Y_val\n",
    "    gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest = tf.train.latest_checkpoint(CHECKPOINTS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_model()\n",
    "#model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loaded from latest checkpoints\n",
    "#model.save(CHECKPOINTS_FOLDER+'/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = tf.keras.models.load_model(CHECKPOINTS_FOLDER+'/my_model.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "#new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21911/21911 [==============================] - 30s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test = test_df.loc[:,'ball_pos_x':'boost5_timer']\n",
    "preds = model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02608083, 0.00367771],\n",
       "       [0.01589172, 0.04301617],\n",
       "       [0.01437513, 0.03655634],\n",
       "       ...,\n",
       "       [0.04801385, 0.00723238],\n",
       "       [0.01273773, 0.021108  ],\n",
       "       [0.06388223, 0.00713387]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>team_A_scoring_within_10sec</th>\n",
       "      <th>team_B_scoring_within_10sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.026081</td>\n",
       "      <td>0.003678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.043016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.036556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>0.029662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.020111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  team_A_scoring_within_10sec  team_B_scoring_within_10sec\n",
       "0   0                     0.026081                     0.003678\n",
       "1   1                     0.015892                     0.043016\n",
       "2   2                     0.014375                     0.036556\n",
       "3   3                     0.015553                     0.029662\n",
       "4   4                     0.026748                     0.020111"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = pd.read_csv(DATA_FOLDER+'/sample_submission.csv')\n",
    "ss['team_A_scoring_within_10sec'] = preds[:,0]\n",
    "ss['team_B_scoring_within_10sec'] = preds[:,1]\n",
    "ss.to_csv(OUT_FOLDER+'/Submission.csv', index=False)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model.history\\ndef plot_graphs(history, metric):\\n    plt.plot(history[metric])\\n    plt.plot(history[f\\'val_{metric}\\'])\\n    plt.xlabel(\"Epochs\")\\n    plt.ylabel(metric)\\n    plt.legend([metric, f\\'val_{metric}\\'])\\n    plt.show()\\n    \\n#plot_graphs(history, \"accuracy\")\\nplot_graphs(history, \"recall\")\\nplot_graphs(history, \"loss\")\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model.history\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history[metric])\n",
    "    plt.plot(history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "#plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"recall\")\n",
    "plot_graphs(history, \"loss\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tabular-playground-series-oct-2022-90FgNoCy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1bd3c623290f22450e25937ea1864e5357b95641848e6b2eee20140815121c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
